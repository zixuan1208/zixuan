{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2",
        "tags": []
      },
      "source": [
        "# **Notebook: Linear regression using gradient descent**\n",
        "\n",
        "In this notebook, you will implement a linear regression model by using gradient descent only using `numpy`. In Hand-in assignment 1, you will later extend this code to a full neural network.\n",
        "\n",
        "**Note:** A linear regression problem can also be solved analytically using the normal equations. However, for the purpose of it being an initial code that you can extend in Hand-in assignment 1, you should solve it here using gradient descent.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LdIDglk1FFcG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIQwgSJ59W8",
        "tags": []
      },
      "source": [
        "Load the dataset `'Auto.csv'`. The dataset:  \n",
        "\n",
        "**Description**:  Gas mileage, horsepower, and other information for 392 vehicles.  \n",
        "**Format**: A data frame with 392 observations on the following 9 variables.  \n",
        "\n",
        "- `mpg`: miles per gallon  \n",
        "- `cylinders`: Number of cylinders between 4 and 8\n",
        "- `displacement`: Engine displacement (cu. inches)\n",
        "- `horsepower`: Engine horsepower\n",
        "- `weight`: Vehicle weight (lbs.)\n",
        "- `acceleration`: Time to accelerate from 0 to 60 mph (sec.)\n",
        "- `year`: Model year (modulo 100)\n",
        "- `origin`: Origin of car (1. American, 2. European, 3. Japanese)\n",
        "- `name`: Vehicle name  \n",
        "*The orginal data contained 408 observations but 16 observations with missing values were removed.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jIfqgJoo6MgD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The null values are '?' in the dataset. `na_values=\"?\"` recognize the null values.\n",
        "# There are null values that will mess up the computation. Easier to drop them by `dropna()`.\n",
        "\n",
        "# import data\n",
        "url = 'https://uu-sml.github.io/course-sml-public/data/auto.csv'\n",
        "auto = pd.read_csv(url, na_values='?', dtype={'ID': str}).dropna().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ukL0wgHhmT",
        "tags": []
      },
      "source": [
        "Familiarize yourself with the dataset using `auto.info()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA8mR7_uHmND",
        "outputId": "e60309da-491a-4024-b2cd-7d169be6c05c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 392 entries, 0 to 391\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   index         392 non-null    int64  \n",
            " 1   mpg           392 non-null    float64\n",
            " 2   cylinders     392 non-null    int64  \n",
            " 3   displacement  392 non-null    float64\n",
            " 4   horsepower    392 non-null    float64\n",
            " 5   weight        392 non-null    int64  \n",
            " 6   acceleration  392 non-null    float64\n",
            " 7   year          392 non-null    int64  \n",
            " 8   origin        392 non-null    int64  \n",
            " 9   name          392 non-null    object \n",
            "dtypes: float64(4), int64(5), object(1)\n",
            "memory usage: 30.8+ KB\n"
          ]
        }
      ],
      "source": [
        "auto.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK4YhRS0HyK6",
        "tags": []
      },
      "source": [
        "We will train a linear regression model with `mpg` as output and the remining features as input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aAOJo_VBHXWD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Extract relevant data features\n",
        "x_train = auto[['cylinders','displacement','horsepower','weight', 'acceleration','year','origin']].values\n",
        "y_train = auto[['mpg']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnUoI0m6GyjC",
        "tags": []
      },
      "source": [
        "\n",
        "Now we are ready to set up the model! The linear regression model is expressed as\n",
        "\\begin{align*}\n",
        "y & = f[\\mathbf x, \\boldsymbol \\phi] \\\\\n",
        "  & = \\sum_{j=1}^{D_i} \\omega_j x_j +   \\beta \\\\\n",
        "  & = \\boldsymbol \\omega^T \\mathbf x + \\beta\n",
        "\\end{align*}\n",
        "where the weights $\\boldsymbol\\omega$ and the bias $\\beta$ are the parameters $\\boldsymbol \\phi =\\{ \\boldsymbol\\omega, \\beta\\}$ of the model.\n",
        "\n",
        "First we initialize all parameters. The weights $\\boldsymbol\\omega$ are stored in  \"weights\" and the bias $\\beta$ is stored in \"bias\". We'll just choose the weights and bias to be equal to zero for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WVM4Tc_jGI0Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def initialize(D_i):\n",
        "    # Arguments\n",
        "    # D_i - Input dimension\n",
        "\n",
        "    # Return\n",
        "    # weights, bias -- Weights and bias for the model\n",
        "\n",
        "    weights = np.zeros((D_i,1))\n",
        "    bias = np.zeros((1,1))\n",
        "\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5irtyxnLJSGX",
        "tags": []
      },
      "source": [
        "Now let's run our random model! We will evaluate the output for all inputs in one go by vectorizing the code. This reads\n",
        "\\begin{align*}\n",
        " \\begin{bmatrix}\n",
        "  f[\\mathbf x_1,\\boldsymbol\\phi] \\\\\n",
        " \\vdots \\\\\n",
        " f[\\mathbf x_I,\\boldsymbol\\phi]\n",
        " \\end{bmatrix}=\n",
        " \\underbrace{\n",
        " \\begin{bmatrix}\n",
        "\t \\mathbf x_1^T \\\\\n",
        "\t \\vdots \\\\\n",
        "\t \\mathbf x_{I}^T \\\\\n",
        " \\end{bmatrix}\n",
        " }_{\\text{net_input}}\n",
        " \\boldsymbol \\omega + \\beta\n",
        " \\end{align*}\n",
        "\n",
        " where $\\beta$ is added to each row (called broadcasting in Python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LgquJUJvJPaN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def forward_pass(net_input, weights, bias):\n",
        "    # Arguments\n",
        "    # net_input -- Input X for the model\n",
        "    # weights, bias -- Weights and bias for the model\n",
        "    #\n",
        "    # Return\n",
        "    # net_output -- Output y of the model\n",
        "\n",
        "    # TODO -- Replace the line below\n",
        "    net_output = np.dot(net_input,weights) + bias\n",
        "\n",
        "    return net_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxVTKp3IcoBF",
        "tags": []
      },
      "source": [
        "Now let's define a cost function.  For this, we'll just use the least squares loss function. We'll also write a function to compute d_cost_d_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6XqWSYWJdhQR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def least_squares_cost(net_output, y):\n",
        "    # Arguments\n",
        "    # net_output -- Output for the model\n",
        "    # y -  true label vector, one-hot encoded\n",
        "    #\n",
        "    # Return\n",
        "    # net_output -- Output y of the model\n",
        "\n",
        "    I = y.shape[0] # Number of data points\n",
        "    cost = np.sum((net_output-y) * (net_output-y))/I\n",
        "    return cost\n",
        "\n",
        "def d_cost_d_output(net_output, y):\n",
        "    # Arguments\n",
        "    # net_output -- Output for the model\n",
        "    # y -  true label vector, one-hot encoded\n",
        "    #\n",
        "    # Return\n",
        "    # d_output -- derivatives of the cost with respect to output of the model\n",
        "\n",
        "    I = y.shape[0] # Number of data points\n",
        "    # TODO Calculate the derivatives of the cost with respect to output of the model, see first item of Preparatory exercise 2.2 and 2.3\n",
        "    d_output = (2*(net_output-y)) / I\n",
        "\n",
        "\n",
        "    return d_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WmyqFYWA-0",
        "tags": []
      },
      "source": [
        "Now let's compute the gradient of the loss with respect to the parameters. To do that we need to do both a forward pass and then a backward pass through the model. We have already done the forward pass.  Let's compute the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LJng7WpRPLMz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Main backward pass routine\n",
        "def backward_pass(weights, bias, net_input, net_output, y):\n",
        "    # Arguments\n",
        "    # weights, bias -- Weights and bias for the model\n",
        "    # net_input, net_output -- Input and output for the model\n",
        "    # y -  true label vector, one-hot encoded\n",
        "    #\n",
        "    # Return\n",
        "    # dl_dweights, dl_dbias -  derivative of cost with respect to weights andb bias\n",
        "\n",
        "    # Compute derivatives of the cost with respect to the network output\n",
        "    dl_df = np.array(d_cost_d_output(net_output,y))\n",
        "\n",
        "    # TODO Calculate the derivatives of the cost with respect to the bias using preparatpry exercise 2.3\n",
        "    # REPLACE THIS LINE\n",
        "    dl_dbias = dl_df\n",
        "\n",
        "    # TODO Calculate the derivatives of the cost with respect to the weights using preparatpry exercise 2.3\n",
        "    # REPLACE THIS LINE\n",
        "    # print(dl_df)\n",
        "    # print(net_input)\n",
        "    dl_dweights = np.dot(net_input.T,dl_df)\n",
        "\n",
        "    return dl_dweights, dl_dbias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mO5ES0cAz28p"
      },
      "source": [
        "We can check we got this right using a trick known as **finite differences**.  If we evaluate the function and then change one of the parameters by a very small amount and normalize by that amount, we get an approximation to the gradient, so:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial \\beta}&\\approx & \\frac{L[\\beta+\\delta, \\omega_1,\\dots,\\omega_{D_i}]-L[\\beta, \\omega_1,\\dots,\\omega_{D_i}]}{\\delta}\\\\\n",
        "\\frac{\\partial L}{\\partial \\omega_{1}}&\\approx & \\frac{L[\\beta, \\omega_1+\\delta,\\dots,\\omega_{D_i}]-L[\\beta, \\omega_1,\\dots,\\omega_{D_i}]}{\\delta}\\\\\n",
        "&\\,\\, \\vdots  \\\\\n",
        "\\frac{\\partial L}{\\partial \\omega_{D_i}}&\\approx & \\frac{L[\\beta, \\omega_1,\\dots,\\omega_{D_i}+\\delta]-L[\\beta, \\omega_1,\\dots,\\omega_{D_i}]}{\\delta}\n",
        "\\end{align}\n",
        "\n",
        "We can't do this when there are many parameters;  for a million parameters, we would have to evaluate the loss function one million plus one times, and usually computing the gradients directly is much more efficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "id": "HkFe3Gqmz28p",
        "outputId": "e5949550-9482-4e2c-8425-59e910bbb98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your derivative of bias\n",
            "[[-0.21641912]\n",
            " [ 0.84593094]\n",
            " [ 0.23000421]\n",
            " [ 0.48597571]\n",
            " [ 0.58007169]\n",
            " [ 0.14880553]\n",
            " [-0.08781366]\n",
            " [-0.60167145]\n",
            " [-0.49478324]\n",
            " [ 0.46642038]]\n",
            "Approximated derivative of bias\n",
            "1.3566209853888012\n",
            "Your derivative of weights\n",
            "[[-3.15653427]\n",
            " [-3.61150844]\n",
            " [ 0.32304185]\n",
            " [ 2.33573436]\n",
            " [-0.31358948]]\n",
            "Approximated derivative of weights\n",
            "[[-3.15636795]\n",
            " [-3.61137937]\n",
            " [ 0.32313799]\n",
            " [ 2.33581887]\n",
            " [-0.3134914 ]]\n"
          ]
        }
      ],
      "source": [
        "# Compute the gradient using your function\n",
        "# Sefine a random\n",
        "D_i = 5\n",
        "I = 10\n",
        "net_input = np.random.normal(size=(I,D_i))\n",
        "y = np.random.normal(size=(I,1))\n",
        "weights = np.random.normal(size=(D_i,1))\n",
        "bias = np.random.normal()\n",
        "\n",
        "\n",
        "# Compute gradient with forward and backward pass\n",
        "net_output = forward_pass(net_input, weights, bias)\n",
        "dl_dweights, dl_dbias = backward_pass(weights, bias, net_input, net_output, y)\n",
        "\n",
        "# Approximate the gradients with finite differences\n",
        "delta = 0.0001\n",
        "\n",
        "# Start with bias\n",
        "bias_delta =  bias + delta\n",
        "net_output_delta = forward_pass(net_input, weights, bias_delta)\n",
        "dl_dbias_est = (least_squares_cost(net_output_delta, y)-least_squares_cost(net_output, y))/delta\n",
        "\n",
        "print(\"Your derivative of bias\")\n",
        "print(dl_dbias)\n",
        "\n",
        "print(\"Approximated derivative of bias\")\n",
        "print(dl_dbias_est)\n",
        "\n",
        "\n",
        "# Now the weights\n",
        "dl_dweights_est  = np.zeros_like(dl_dweights)\n",
        "\n",
        "for j in range(D_i):\n",
        "    weights_delta = np.array(weights)\n",
        "    weights_delta[j] += delta\n",
        "    net_output_delta = forward_pass(net_input, weights_delta, bias)\n",
        "    dl_dweights_est[j] = (least_squares_cost(net_output_delta, y)-least_squares_cost(net_output, y))/delta\n",
        "\n",
        "print(\"Your derivative of weights\")\n",
        "print(dl_dweights)\n",
        "\n",
        "print(\"Approximated derivative of weights\")\n",
        "print(dl_dweights_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKbaO8FUCm7D",
        "tags": []
      },
      "source": [
        "Once we have checked that the derivatives match we can proceed.\n",
        "\n",
        "We also need a function for how to update the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK-UtE3hreAK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def update_parameters(weights, bias, dl_dweights, dl_dbias, alpha):\n",
        "  # Arguments\n",
        "  # weights, bias -- Weights and bias for the model\n",
        "  # dl_dweights, dl_dbias -  derivative of loss with respect to weights and bias\n",
        "  # alpha -  learning rate\n",
        "  #\n",
        "  # Return\n",
        "  # weights, bias -- Updated weights and bias for the model\n",
        "\n",
        "  # TODO -- Replace the lines below\n",
        "  weights = weights\n",
        "  bias = bias\n",
        "\n",
        "  return weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUZcv1ikC2sj",
        "tags": []
      },
      "source": [
        "Here we have the main function for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ZKLVooUwJG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_model(x_train,y_train,iterations,alpha):\n",
        "  # Arguments\n",
        "  # x, y -- Input and output for training data\n",
        "  # iterations -- number of iterations\n",
        "  # alpha -- learning rate\n",
        "\n",
        "  D_i = x_train.shape[1]\n",
        "  weights, bias = initialize(D_i)\n",
        "\n",
        "  train_costs, test_costs, train_accuracies, test_accuracies = [], [], [], []\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "\n",
        "    # Forward pass\n",
        "    net_output = forward_pass(x_train,weights, bias)\n",
        "\n",
        "    # Backward pass\n",
        "    dl_dweights, dl_dbias = backward_pass(weights, bias, x_train, net_output, y_train)\n",
        "\n",
        "    # Update parameters\n",
        "    weights, bias = update_parameters(weights, bias, dl_dweights, dl_dbias, alpha)\n",
        "\n",
        "    # For every iteration compute loss\n",
        "    if iteration % 1 == 0:\n",
        "      train_cost = least_squares_cost(net_output, y_train)\n",
        "      #print(\"Iteration %i: Train cost: %f.\" %(iteration, train_cost))\n",
        "      train_costs.append(train_cost)\n",
        "\n",
        "  train_costs = np.array(train_costs)\n",
        "  return train_costs, weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkN_weOCKRu9",
        "tags": []
      },
      "source": [
        "Function for doing the plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDDOTQNRlMOH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def training_curve_plot(train_costs):\n",
        "  \"\"\" convenience function for plotting train cost\n",
        "  \"\"\"\n",
        "  lg=13\n",
        "  md=10\n",
        "  sm=9\n",
        "  x = range(1, len(train_costs)+1)\n",
        "  plt.plot(x, train_costs, label=f'Final train cost: {train_costs[-1]:.4f}')\n",
        "  plt.xlabel('Iteration', fontsize=md)\n",
        "  plt.ylabel('Cost', fontsize=md)\n",
        "  plt.legend(fontsize=sm)\n",
        "  plt.tick_params(axis='both', labelsize=sm)\n",
        "  plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
        "  plt.show()\n",
        "\n",
        "def abline(slope, intercept):\n",
        "  \"\"\"Plot a line from slope and intercept\"\"\"\n",
        "  axes = plt.gca()\n",
        "  x_vals = np.array(axes.get_xlim())\n",
        "  y_vals = intercept + slope * x_vals\n",
        "  plt.plot(x_vals, y_vals, '--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rEiDnFKVk9",
        "tags": []
      },
      "source": [
        "For gradient descent to work properly it is important to normalze the data. This can be done, for example, by subtracting the mean and dividing with the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydzhRJY3D7Td",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Normalize the input data\n",
        "x_train_norm = (x_train - np.mean(x_train,axis=0))/np.std(x_train,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe9E2Kd8LYDD",
        "tags": []
      },
      "source": [
        "Now we are ready to train the model! Fit **two** linear regression models for the two choices of inputs ((i) only `horsepower` and (ii) all except `name`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hlPpCbzpz28p"
      },
      "outputs": [],
      "source": [
        "# Train the model with horsepower only as input\n",
        "train_losses, weights, bias = train_model(x_train_norm[:,[2]],y_train,iterations=100,alpha=0.1)\n",
        "training_curve_plot(train_losses)\n",
        "# Plot the linear regression model together with the data\n",
        "plt.plot(x_train_norm[:,2],y_train,'.')\n",
        "abline(weights[0,0], bias[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCMm5ElFirvX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the model with all features\n",
        "train_costs, weights, bias = train_model(x_train_norm,y_train,iterations=100,alpha=0.1)\n",
        "training_curve_plot(train_costs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBdamdHag2ic",
        "tags": []
      },
      "source": [
        "With a correct implementation, the final cost for the two models should be less than 25 and less than 12, respectively.\n",
        "\n",
        "**TO DO:**  State the final loss for each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca31yizDfHKe",
        "tags": []
      },
      "source": [
        "**TO DO:** To study the relationship between *learning rate* and *number of iterations*, try learning rates `[1, 1e-1, 1e-2, 1e-3, 1e-4]` for the two models. Provide a high level interpretation of what you observe evaluating these learning rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj8VWLwtgl6i",
        "tags": []
      },
      "source": [
        "**TO DO:** Try repeating any of the previous experiments without normalizing the input. What happens then? Can you find a learning rate sch that we converge?"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Tags",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}